{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ad4eb9",
   "metadata": {},
   "source": [
    "# Implementing Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c4bf7-1c88-45e7-b197-42812d5f3baa",
   "metadata": {},
   "source": [
    "#### Installing essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c7b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f9e4e-7e26-4737-abf8-8008b766624c",
   "metadata": {},
   "source": [
    "#### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d0161-7268-4778-b665-7b17894d6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:/Users/riddh/OneDrive/Documents/Projects/Churn/Preprocessed_data.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fcf318-ee6e-4f79-8623-1568fc4328ab",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e9c09-9b76-4e44-9464-0fe1733f2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d0f37-2cc1-4d5d-bd02-d1fa652f3df6",
   "metadata": {},
   "source": [
    "#### Split data into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244aa749-cd0e-4aae-a4cf-885c9321b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f87d9ee-99cb-417b-bd6a-611dff53d994",
   "metadata": {},
   "source": [
    "#### Check the dimension of the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train:',np.shape(X_train))\n",
    "print('y_train:',np.shape(y_train))\n",
    "print('X_test:',np.shape(X_test))\n",
    "print('y_test:',np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec2055-31f3-44ef-9017-6cef02ae35bf",
   "metadata": {},
   "source": [
    "#### Checking Distribution of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165aa12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41677d3-42f0-4e79-895b-6df840ca8850",
   "metadata": {},
   "source": [
    "#### Reshaping of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5fa3f-edd5-47e2-9307-9da1f2e2d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = np.array(X_val).reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257be91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of the sets\n",
    "print('X_train:', np.shape(X_train))\n",
    "print('y_train:', np.shape(y_train))\n",
    "print('X_val:', np.shape(X_val))\n",
    "print('y_val:', np.shape(y_val))\n",
    "print('X_test:', np.shape(X_test))\n",
    "print('y_test:', np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b02be-1333-4c7d-99ad-fdce85bfd279",
   "metadata": {},
   "source": [
    "#### This code converts the X_train data by first cleaning it to replace any boolean values with integers, then converting the cleaned data into a NumPy array of type float32, and finally converting that array into a TensorFlow tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f4fca-e9e8-4ec3-b6cd-d4bcc06a2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cleaned = np.array([[int(x) if isinstance(x, bool) else x for x in row] for row in X_train])\n",
    "X_train_cleaned = np.array(X_train_cleaned, dtype=np.float32)\n",
    "X_train = tf.convert_to_tensor(X_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a347300-1d88-4ab5-b705-eb7fcb40ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_cleaned = np.array([[int(x) if isinstance(x, bool) else x for x in row] for row in X_val])\n",
    "X_val_cleaned = np.array(X_val_cleaned, dtype=np.float32)\n",
    "X_val = tf.convert_to_tensor(X_val_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf2dbd-83a5-414c-ae50-32993691b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cleaned = np.array([[int(x) if isinstance(x, bool) else x for x in row] for row in X_test])\n",
    "X_test_cleaned = np.array(X_test_cleaned, dtype=np.float32)\n",
    "X_test = tf.convert_to_tensor(X_test_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961acce6",
   "metadata": {},
   "source": [
    "#### Model Designing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f40c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=3, activation='sigmoid', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='sigmoid'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987e5c2-d343-4c9e-a4d9-18df700ca9d7",
   "metadata": {},
   "source": [
    "#### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a summary of the model architecture including the types of layers,\n",
    "#output shapes, and the number of parameters in each layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0622df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Fit Model\n",
    "epochs = 100\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, batch_size=32, \n",
    "                   callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3df41",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3229706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test_prob = model.predict(X_test)\n",
    "yhat_test = (yhat_test_prob > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "conf_matrix = confusion_matrix(y_test, yhat_test)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, yhat_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Printing the accuracy score\n",
    "print('Accuracy:')\n",
    "print(float(accuracy_score(y_test, yhat_test))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the keys of the history object returned during model training\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659a319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the epoch range based on the length of the loss history\n",
    "epoch_range = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "# Plot the training and validation loss over epochs\n",
    "plt.plot(epoch_range, history.history['loss'], label='Train')   # Training loss\n",
    "plt.plot(epoch_range, history.history['val_loss'], label='Validation')  # Validation loss\n",
    "\n",
    "# Adding labels and title to the plot\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.xlim((1, len(history.history['loss'])))  # Ensure x-axis limit matches the number of epochs\n",
    "plt.legend()\n",
    "plt.title('Loss vs Epochs')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb47042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the epoch range based on the length of the accuracy history\n",
    "epoch_range = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "# Plot the training and validation accuracy over epochs\n",
    "plt.plot(epoch_range, history.history['accuracy'], label='Train')   # Training accuracy\n",
    "plt.plot(epoch_range, history.history['val_accuracy'], label='Validation')  # Validation accuracy\n",
    "\n",
    "# Adding labels and title to the plot\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.xlim((1, len(history.history['accuracy'])))  # Ensure x-axis limit matches the number of epochs\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs Epochs')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
